# WIKIPT-2
A GPT-2 model trained on manually selected Wikipedia articles. Generate Wikipedia articles with GPT-2

- Trained on manually selected Wikipedia article source codes.
    - Trained up to 100K iterations.
    - Trained on a Titan RTX.
- About 1GB big, download below.
- Licensed under MIT.

## Download
Download from my [google drive](https://drive.google.com/drive/folders/1JL3jfEeIMcLhsG453EJfLMCQ3u8znmcx).

## Use
No support is provided. Functional with this repository: https://github.com/nshepperd/gpt-2